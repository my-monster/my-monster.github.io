(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{341:function(s,e,n){"use strict";n.r(e);var a=n(4),t=Object(a.a)({},(function(){var s=this,e=s._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("h3",{attrs:{id:"torch-optim-lr-scheduler-multisteplr-optimizer-milestones-gamma-0-1-last-epoch-1-verbose-false"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#torch-optim-lr-scheduler-multisteplr-optimizer-milestones-gamma-0-1-last-epoch-1-verbose-false"}},[s._v("#")]),s._v(" "),e("code",[s._v("torch.optim.lr_scheduler.MultiStepLR")]),s._v("("),e("em",[s._v("optimizer")]),s._v(", "),e("em",[s._v("milestones")]),s._v(", "),e("em",[s._v("gamma=0.1")]),s._v(", "),e("em",[s._v("last_epoch=-1")]),s._v(", "),e("em",[s._v("verbose=False")]),s._v(")")]),s._v(" "),e("h2",{attrs:{id:"一、结论"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一、结论"}},[s._v("#")]),s._v(" 一、结论：")]),s._v(" "),e("ol",[e("li",[e("strong",[s._v("last_epoch的用法")]),s._v("：last_epoch表示已经走了多少个epoch，下一个milestone减去last_epoch就是需要的epoch数"),e("br"),s._v("\n（评论区原话：last_epoch是有用的，简单来说，就是所有学习率都要提前last_epoch开始进行变化。举个例子假如我设置原始lr=0.1，milestones=[5, 15], gamma=0.5，last_epoch=0此时epoch=5时lr才会变为0.05，epoch=15时lr变为0.025。当修改last_epoch=3后，epoch=2，lr就会变为0.05，epoch=12，lr变为0.025.一般默认last_epoch=0）")]),s._v(" "),e("li",[e("strong",[s._v("使用scheduler.get_lr()，会在milestone的时候乘以gamma的平方")])]),s._v(" "),e("li",[e("strong",[s._v("新版本pytorch已经有了新的变化：")]),e("br"),s._v('\n新版的pytorch没有get_lr()函数了，应该用get_last_lr()代替get_lr()，而且 get_last_lr() 也没有 "乘gamma平方" 这个问题了')])]),s._v(" "),e("h2",{attrs:{id:"二、实验代码如下"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二、实验代码如下"}},[s._v("#")]),s._v(" 二、实验代码如下：")]),s._v(" "),e("h3",{attrs:{id:"_1、首先是默认配置"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、首先是默认配置"}},[s._v("#")]),s._v(" 1、首先是默认配置：")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torchvision\n \nlearing_rate = 0.1\nmodel = torchvision.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=learing_rate,\n                                momentum=0.9,\n                                weight_decay=5e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6], gamma=0.1)\n \nfor epoch in range(9):\n    optimizer.step()\n    scheduler.step()\n    # print(optimizer.get_lr())\n    print(epoch, scheduler.get_lr())\n \n返回：\n0 [0.1]\n1 [0.1]\n2 [0.0010000000000000002]   # 此处乘的是gamma的平方\n3 [0.010000000000000002]\n4 [0.010000000000000002]\n5 [0.00010000000000000003]   # 此处乘的是gamma的平方\n6 [0.0010000000000000002]\n7 [0.0010000000000000002]\n8 [0.0010000000000000002]\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br")])]),e("h3",{attrs:{id:"_2、设置last-epoch-1。"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、设置last-epoch-1。"}},[s._v("#")]),s._v(" 2、设置last_epoch=-1。")]),s._v(" "),e("p",[s._v("和前面1、是一样的，因为函数的默认值就是last_epoch=-1")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torchvision\n \nlearing_rate = 0.1\nmodel = torchvision.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=learing_rate,\n                                momentum=0.9,\n                                weight_decay=5e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6], gamma=0.1)\nscheduler.last_epoch = -1\n \nfor epoch in range(9):\n    optimizer.step()\n    scheduler.step()\n    # print(optimizer.get_lr())\n    print(epoch, scheduler.get_lr())\n \n返回：\n0 [0.1]\n1 [0.1]\n2 [0.1]\n3 [0.0010000000000000002]\n4 [0.010000000000000002]\n5 [0.010000000000000002]\n6 [0.00010000000000000003]\n7 [0.0010000000000000002]\n8 [0.0010000000000000002]\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br")])]),e("h3",{attrs:{id:"_3、设置last-epoch-4。"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3、设置last-epoch-4。"}},[s._v("#")]),s._v(" 3、设置last_epoch=4。")]),s._v(" "),e("p",[s._v("把第1个epoch的learning_rate设置为0.1，但是按照模型已经更新到了第4个epoch开始执行。")]),s._v(" "),e("p",[s._v("后来理解了一下，感觉就是：在last_epoch处将learning_rate重新设置为初始值，而且也是从last_epoch处继续进行运行；所以就"),e("strong",[s._v("要求你手动把learning_rate设为上一次模型停止的时候对应的learning_rate值，即last_epoch处对应的learning_rate")]),s._v("。")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torchvision\n \nlearing_rate = 0.1\nmodel = torchvision.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=learing_rate,\n                                momentum=0.9,\n                                weight_decay=5e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6], gamma=0.1)\nscheduler.last_epoch = 4\n \nfor epoch in range(9):\n    optimizer.step()\n    scheduler.step()\n    # print(optimizer.get_lr())\n    print(epoch, scheduler.get_lr())\n \n返回：\n0 [0.1] # 0相当于第4个eopch\n1 [0.0010000000000000002]  # 1相当于第5个epoch所以乘以gamma的平方\n2 [0.010000000000000002]\n3 [0.010000000000000002]\n4 [0.010000000000000002]\n5 [0.010000000000000002]\n6 [0.010000000000000002]\n7 [0.010000000000000002]\n8 [0.010000000000000002]  # 因为4在3的后面，只有一个6这个milestones，所以只更新了一次\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br")])]),e("h3",{attrs:{id:"_4、设置last-epoch-4-并且将scheduler-step-改为schduler-step-epoch-。也是不对"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4、设置last-epoch-4-并且将scheduler-step-改为schduler-step-epoch-。也是不对"}},[s._v("#")]),s._v(" 4、设置last_epoch=4，并且将scheduler.step()改为schduler.step(epoch)。也是不对")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torchvision\n \nlearing_rate = 0.1\nmodel = torchvision.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=learing_rate,\n                                momentum=0.9,\n                                weight_decay=5e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6], gamma=0.1)\nscheduler.last_epoch = 4\n \nfor epoch in range(9):\n    optimizer.step()\n    scheduler.step(epoch)\n    # print(optimizer.get_lr())\n    print(epoch, scheduler.get_lr())\n \n返回：\n0 [0.0010000000000000002]\n1 [0.0010000000000000002]\n2 [0.0010000000000000002]\n3 [0.00010000000000000003]\n4 [0.0010000000000000002]\n5 [0.0010000000000000002]\n6 [0.00010000000000000003]\n7 [0.0010000000000000002]\n8 [0.0010000000000000002]\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br")])]),e("h3",{attrs:{id:"我试过了-无论把last-epoch改为多少-输出都是上面这个。证明scheduler-step-里面是一定不能加epoch的"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#我试过了-无论把last-epoch改为多少-输出都是上面这个。证明scheduler-step-里面是一定不能加epoch的"}},[s._v("#")]),s._v(" 我试过了，无论把last_epoch改为多少，输出都是上面这个。证明scheduler.step()里面是一定不能加epoch的")]),s._v(" "),e("h3",{attrs:{id:"_5、接3-当last-epoch大于milestones的某些值时-会自动跳过这些值"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5、接3-当last-epoch大于milestones的某些值时-会自动跳过这些值"}},[s._v("#")]),s._v(" "),e("strong",[s._v("5、接3，当last_epoch大于milestones的某些值时，会自动跳过这些值")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torchvision\n \nlearing_rate = 0.1\nmodel = torchvision.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=learing_rate,\n                                momentum=0.9,\n                                weight_decay=5e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 8], gamma=0.1)\nscheduler.last_epoch = 4\n \nfor epoch in range(9):\n    optimizer.step()\n    # scheduler.step(epoch)\n    scheduler.step()\n    # print(optimizer.get_lr())\n    print(epoch, scheduler.get_lr())\n \n返回：\n0 [0.0010000000000000002]\n1 [0.010000000000000002]\n2 [0.010000000000000002]\n3 [0.00010000000000000003]\n4 [0.0010000000000000002]\n5 [0.0010000000000000002]\n6 [0.0010000000000000002]\n7 [0.0010000000000000002]\n8 [0.0010000000000000002]\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br")])]),e("h3",{attrs:{id:"对比第3小节和第5小节的例子可以发现-在3中的例子中-4大于3-所以把3跳过了-直接在milestone-6的时候调整的learning-rate"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#对比第3小节和第5小节的例子可以发现-在3中的例子中-4大于3-所以把3跳过了-直接在milestone-6的时候调整的learning-rate"}},[s._v("#")]),s._v(" 对比第3小节和第5小节的例子可以发现，在3中的例子中，4大于3，所以把3跳过了，直接在milestone=6的时候调整的learning_rate")]),s._v(" "),e("h2",{attrs:{id:"三、新版本pytorch没有get-lr-这个函数了-用get-last-lr-代替"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三、新版本pytorch没有get-lr-这个函数了-用get-last-lr-代替"}},[s._v("#")]),s._v(" 三、新版本pytorch没有get_lr()这个函数了，用get_last_lr()代替")]),s._v(" "),e("h3",{attrs:{id:"_1、官方文档"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、官方文档"}},[s._v("#")]),s._v(" 1、官方文档：")]),s._v(" "),e("blockquote",[e("p",[e("strong",[e("code",[s._v("get_last_lr")]),s._v("()：")])]),s._v(" "),e("p",[s._v("Return the last computed learning rate by current "),e("a",{attrs:{href:"https://so.csdn.net/so/search?q=scheduler&spm=1001.2101.3001.7020",target:"_blank",rel:"noopener noreferrer"}},[s._v("scheduler"),e("OutboundLink")],1),s._v(".（获取scheduler计算的最后学习速率）")]),s._v(" "),e("p",[e("strong",[e("code",[s._v("print_lr")]),s._v("("),e("em",[s._v("is_verbose")]),s._v(", "),e("em",[s._v("group")]),s._v(", "),e("em",[s._v("lr")]),s._v(", "),e("em",[s._v("epoch=None")]),s._v(")：# 这个怎么用，还没空研究")])]),s._v(" "),e("p",[s._v("Display the current learning rate.（返回当前的学习率）")]),s._v(" "),e("p",[e("strong",[s._v("看来，现在没有get_lr()这个函数了")])])]),s._v(" "),e("h3",{attrs:{id:"_2、例子代码"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、例子代码"}},[s._v("#")]),s._v(" 2、例子代码：")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torchvision\n \nlearing_rate = 0.1\nmodel = torchvision.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=learing_rate,\n                                momentum=0.9,\n                                weight_decay=5e-5)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6], gamma=0.1)\n \nfor epoch in range(9):\n    optimizer.step()\n    scheduler.step()\n    # print(optimizer.get_lr())\n    print(epoch, scheduler.get_last_lr())\n \n \n \n返回：\n0 [0.1]\n1 [0.1]\n2 [0.010000000000000002]\n3 [0.010000000000000002]\n4 [0.010000000000000002]\n5 [0.0010000000000000002]\n6 [0.0010000000000000002]\n7 [0.0010000000000000002]\n8 [0.0010000000000000002]\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br")])]),e("p",[s._v("可以看到：没有“乘以gamma平方”这个问题了。")]),s._v(" "),e("p",[s._v("ps: 是评论区的 angleboy8 朋友发现的。")])])}),[],!1,null,null,null);e.default=t.exports}}]);