(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{340:function(s,e,t){"use strict";t.r(e);var n=t(4),a=Object(n.a)({},(function(){var s=this,e=s._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/b9aa14271d734c1ac17022265360ae91.jpeg#pic_center",alt:"https://pica.zhimg.com/v2-a702c81aaaa2f6c7c417cb94f87f496f_1440w.jpg?source=172ae18b"}})]),s._v(" "),e("p",[s._v("最近在用mmdection框架修改网络的时候发现，网络训练起来一直都不收敛，训练一小会就换全部变nan，检测了好久都没有发现什么问题，最终修改了学习率，终于可以收敛了。但是关于怎么调整学习率一直都还没有掌握。因此特意写了这一篇进行总结。")]),s._v(" "),e("h2",{attrs:{id:"_1-optimizer-step-和scheduler-step-的区别"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-optimizer-step-和scheduler-step-的区别"}},[s._v("#")]),s._v(" "),e("strong",[s._v("1.optimizer.step()和scheduler.step()的区别")])]),s._v(" "),e("p",[s._v("optimizer.step()和scheduler.step()是我们在训练网络之前都需要设置。我理解的是optimizer是指定"),e("strong",[s._v("使用哪个优化器")]),s._v("，scheduler是"),e("strong",[s._v("对优化器的学习率进行调整")]),s._v("，正常情况下训练的步骤越大，学习率应该变得越小。optimizer.step()通常用在每个mini-batch之中，而scheduler.step()通常用在epoch里面,但是不绝对。可以根据具体的需求来做。只有用了optimizer.step()，模型才会更新，而scheduler.step()是对lr进行调整。通常我们在scheduler的step_size表示scheduler.step()每调用step_size次，对应的学习率就会按照策略调整一次。所以如果scheduler.step()是放在mini-batch里面，那么step_size指的是经过这么多次迭代，学习率改变一次。下面为一个简单的使用实例：")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\nscheduler = lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\nmodel = net.train(model, loss_function, optimizer, scheduler, num_epochs = 100)\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br")])]),e("h2",{attrs:{id:"_2-1optimizer的种类"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1optimizer的种类"}},[s._v("#")]),s._v(" 2.1optimizer的种类")]),s._v(" "),e("h3",{attrs:{id:"_2-1-optim-sgd"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-optim-sgd"}},[s._v("#")]),s._v(" 2.1 optim.SGD")]),s._v(" "),e("h3",{attrs:{id:"_2-2-optim-adam"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-optim-adam"}},[s._v("#")]),s._v(" 2.2 optim.Adam")]),s._v(" "),e("p",[s._v("pytorch有torch.optim.lr_scheduler模块提供了一些根据epoch训练次数来调整学习率（learning rate）的方法。一般情况下我们会设置随着epoch的增大而逐渐减小学习率从而达到更好的训练效果。学习率的调整应该放在optimizer更新之后，下面是一个参考伪代码：")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("scheduler = ...\nfor epoch in range(100):\n     train(...)\n     validate(...)\n     scheduler.step()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br")])]),e("p",[s._v("本文介绍的调整学习率的函数都是基于epoch大小变化进行调整的。")]),s._v(" "),e("h3",{attrs:{id:"_3-1torch-optim-lr-scheduler-lambdalr"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1torch-optim-lr-scheduler-lambdalr"}},[s._v("#")]),s._v(" 3.1torch.optim.lr_scheduler.LambdaLR")]),s._v(" "),e("blockquote",[e("p",[s._v("class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)")])]),s._v(" "),e("p",[s._v("学习率的更新公式为：  $\\text { new_l } r=\\lambda \\times \\text { initial_l } r$\\text { new_l } r=\\lambda \\times \\text { initial_l } r")]),s._v(" "),e("p",[s._v("$\\text { new_l } r$\\text { new_l } r 是得到的新的学习率，  $\\text { initial_l } r$\\text { initial_l } r 是初始的学习率，λ是通过参数lr_lambda和epoch得到的。")]),s._v(" "),e("ul",[e("li",[s._v("optimizer （Optimizer）：要更改学习率的优化器；")]),s._v(" "),e("li",[s._v("lr_lambda（function or list）：根据epoch计算λ的函数；或者是一个list的这样的function，分别计算各个parameter groups的学习率更新用到的λ；")]),s._v(" "),e("li",[s._v("last_epoch （int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import LambdaLR\n\ninitial_lr = 0.1\nnet_1 = model()\n\noptimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\nscheduler_1 = LambdaLR(optimizer_1, lr_lambda=lambda epoch: 1/(epoch+1))\n\nprint(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n\nfor epoch in range(1, 11):\n    # train\n    optimizer_1.zero_grad()\n    optimizer_1.step()\n    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n    scheduler_1.step()\n\n初始化的学习率： 0.1\n第1个epoch的学习率：0.100000\n第2个epoch的学习率：0.050000\n第3个epoch的学习率：0.033333\n第4个epoch的学习率：0.025000\n第5个epoch的学习率：0.020000\n第6个epoch的学习率：0.016667\n第7个epoch的学习率：0.014286\n第8个epoch的学习率：0.012500\n第9个epoch的学习率：0.011111\n第10个epoch的学习率：0.010000\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br")])]),e("h3",{attrs:{id:"_3-2-torch-optim-lr-scheduler-steplr"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-torch-optim-lr-scheduler-steplr"}},[s._v("#")]),s._v(" 3.2 torch.optim.lr_scheduler.StepLR")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("p",[s._v("学习率的更新公式为：  $\\text { new_l } r=\\text { initial_l } r \\times \\gamma^{\\text {epoch } / / \\text { step_size }}$ \\text { new_l } r=\\text { initial_l } r \\times \\gamma^{\\text {epoch } / / \\text { step_size }}")]),s._v(" "),e("p",[s._v("参数：")]),s._v(" "),e("ul",[e("li",[s._v("optimizer （Optimizer）：要更改学习率的优化器；")]),s._v(" "),e("li",[s._v("step_size（int）：每训练step_size个epoch，更新一次参数；")]),s._v(" "),e("li",[s._v("gamma（float）：更新lr的乘法因子；")]),s._v(" "),e("li",[s._v("last_epoch （int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import StepLR\n\ninitial_lr = 0.1\nnet_1 = model()\n\noptimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\nscheduler_1 = StepLR(optimizer_1, step_size=3, gamma=0.1)\n\nprint(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n\nfor epoch in range(1, 11):\n    # train\n    optimizer_1.zero_grad()\n    optimizer_1.step()\n    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n    scheduler_1.step()\n\n初始化的学习率： 0.1\n第1个epoch的学习率：0.100000\n第2个epoch的学习率：0.100000\n第3个epoch的学习率：0.100000\n第4个epoch的学习率：0.010000\n第5个epoch的学习率：0.010000\n第6个epoch的学习率：0.010000\n第7个epoch的学习率：0.001000\n第8个epoch的学习率：0.001000\n第9个epoch的学习率：0.001000\n第10个epoch的学习率：0.000100\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br")])]),e("h3",{attrs:{id:"_3-3-torch-optim-lr-scheduler-multisteplr"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-torch-optim-lr-scheduler-multisteplr"}},[s._v("#")]),s._v(" 3.3 torch.optim.lr_scheduler.MultiStepLR")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("p",[s._v("学习率的更新公式为： $n e w_{-l} r=\\text { initial_lr } \\times \\gamma^{\\text {bisect_right }(\\text { milestones,epoch })}$n e w_{-l} r=\\text { initial_lr } \\times \\gamma^{\\text {bisect_right }(\\text { milestones,epoch })}")]),s._v(" "),e("p",[s._v("参数：")]),s._v(" "),e("ul",[e("li",[s._v("optimizer （Optimizer）：要更改学习率的优化器；")]),s._v(" "),e("li",[s._v("milestones（list）：递增的list，存放要更新lr的epoch；")]),s._v(" "),e("li",[s._v("gamma（float）：更新lr的乘法因子；")]),s._v(" "),e("li",[s._v("last_epoch （int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import MultiStepLR\n\ninitial_lr = 0.1\nnet_1 = model()\n\noptimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\nscheduler_1 = MultiStepLR(optimizer_1, milestones=[3, 7], gamma=0.1)\n\nprint(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n\nfor epoch in range(1, 11):\n    # train\n    optimizer_1.zero_grad()\n    optimizer_1.step()\n    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n    scheduler_1.step()\n\n初始化的学习率： 0.1\n第1个epoch的学习率：0.100000\n第2个epoch的学习率：0.100000\n第3个epoch的学习率：0.100000\n第4个epoch的学习率：0.010000\n第5个epoch的学习率：0.010000\n第6个epoch的学习率：0.010000\n第7个epoch的学习率：0.010000\n第8个epoch的学习率：0.001000\n第9个epoch的学习率：0.001000\n第10个epoch的学习率：0.001000\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br")])]),e("h3",{attrs:{id:"_3-4-torch-optim-lr-scheduler-exponentiallr"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-torch-optim-lr-scheduler-exponentiallr"}},[s._v("#")]),s._v(" 3.4 torch.optim.lr_scheduler.ExponentialLR")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("p",[s._v("学习率的更新公式为： $n e w_{-l} r=\\text { initial_l } r \\times \\gamma^{\\text {epoch }}$ n e w_{-l} r=\\text { initial_l } r \\times \\gamma^{\\text {epoch }}")]),s._v(" "),e("p",[s._v("参数：")]),s._v(" "),e("ul",[e("li",[s._v("optimizer （Optimizer）：要更改学习率的优化器；")]),s._v(" "),e("li",[s._v("gamma（float）：更新lr的乘法因子；")]),s._v(" "),e("li",[s._v("last_epoch （int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ExponentialLR\n\ninitial_lr = 0.1\nnet_1 = model()\n\noptimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\nscheduler_1 = ExponentialLR(optimizer_1, gamma=0.1)\n\nprint(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n\nfor epoch in range(1, 11):\n    # train\n    optimizer_1.zero_grad()\n    optimizer_1.step()\n    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n    scheduler_1.step()\n\n初始化的学习率： 0.1\n第1个epoch的学习率：0.100000\n第2个epoch的学习率：0.010000\n第3个epoch的学习率：0.001000\n第4个epoch的学习率：0.000100\n第5个epoch的学习率：0.000010\n第6个epoch的学习率：0.000001\n第7个epoch的学习率：0.000000\n第8个epoch的学习率：0.000000\n第9个epoch的学习率：0.000000\n第10个epoch的学习率：0.000000\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br")])]),e("p",[s._v("参考资料：")]),s._v(" "),e("p",[s._v("1."),e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/344294796",target:"_blank",rel:"noopener noreferrer"}},[s._v("训练时的学习率调整：optimizer和scheduler"),e("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=a.exports}}]);